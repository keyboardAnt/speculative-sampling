{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nadavt/opt/anaconda3/envs/jaymody-sps-env/bin/python\n",
      "Python 3.9.10\n",
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!python --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m torch.utils.collect_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate via `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadavt/opt/anaconda3/envs/jaymody-sps-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50257])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model & Sanity check\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "draft_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "target_model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "draft_inputs = draft_tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "target_inputs = target_tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# Assert that `draft_tokenizer` and `target_tokenizer` are equivalent\n",
    "assert torch.all(draft_inputs.input_ids == target_inputs.input_ids)\n",
    "assert torch.all(draft_inputs.attention_mask == target_inputs.attention_mask)\n",
    "\n",
    "draft_outputs = draft_model(**draft_inputs, labels=draft_inputs[\"input_ids\"])\n",
    "target_outputs = target_model(**target_inputs, labels=target_inputs[\"input_ids\"])\n",
    "\n",
    "\n",
    "loss = draft_outputs.loss\n",
    "logits = draft_outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draft_outputs.sequences == target_outputs.sequences:\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False]])\n",
      "draft_outputs.sequences:\n",
      "[\"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\"]\n",
      "target_outputs.sequences:\n",
      "[\"Hello, my dog is cute. I'm going to take him to the park today. I'm going to take him to the\"]\n",
      "draft_outputs:\n",
      "GreedySearchDecoderOnlyOutput(sequences=tensor([[15496,    11,   616,  3290,   318, 13779,    13,   314,  1101,   407,\n",
      "          1654,   611,   673,   338,   257, 26188,   393,   407,    13,   314,\n",
      "          1101,   407,  1654,   611,   673,   338]]), scores=(tensor([[-77.4425, -80.4463, -88.0498,  ..., -96.2564, -93.6345, -84.0666]]), tensor([[-142.3193, -142.1797, -144.0388,  ..., -155.6977, -153.8761,\n",
      "         -137.7942]]), tensor([[-175.1097, -174.3555, -180.2891,  ..., -186.5560, -176.5305,\n",
      "         -176.7951]]), tensor([[-149.5665, -149.3980, -153.9603,  ..., -158.9080, -155.3729,\n",
      "         -151.3569]]), tensor([[-146.1682, -147.2101, -157.6489,  ..., -158.1570, -155.2079,\n",
      "         -150.0087]]), tensor([[-139.9639, -140.1997, -149.4295,  ..., -151.1516, -150.9630,\n",
      "         -143.5753]]), tensor([[-111.7725, -111.0601, -118.1015,  ..., -119.4616, -118.6231,\n",
      "         -113.0703]]), tensor([[-144.3919, -142.3804, -151.1539,  ..., -153.2746, -146.0315,\n",
      "         -144.4816]]), tensor([[-141.3286, -140.4888, -146.7292,  ..., -147.8345, -143.9044,\n",
      "         -142.7541]]), tensor([[-117.3899, -116.4772, -121.3260,  ..., -126.3711, -123.0911,\n",
      "         -117.4414]]), tensor([[ -98.5157,  -99.3022, -105.9519,  ..., -112.5625, -109.3629,\n",
      "         -103.0630]]), tensor([[-106.7821, -107.9948, -112.8315,  ..., -116.4186, -113.5325,\n",
      "         -107.8266]]), tensor([[-45.3370, -46.1747, -54.2726,  ..., -61.4791, -59.7055, -50.6116]]), tensor([[-162.0715, -160.2719, -162.9900,  ..., -174.5018, -174.7954,\n",
      "         -156.2698]]), tensor([[-181.4132, -180.2989, -185.5506,  ..., -191.4404, -182.3735,\n",
      "         -182.6374]]), tensor([[-157.1393, -156.6873, -161.9810,  ..., -163.8544, -162.9273,\n",
      "         -158.6823]]), tensor([[-147.5977, -148.1111, -157.4325,  ..., -156.5320, -155.3785,\n",
      "         -150.3270]]), tensor([[-135.2703, -136.3291, -145.7141,  ..., -147.2964, -147.4553,\n",
      "         -139.9984]]), tensor([[-103.0535, -102.4292, -109.0970,  ..., -110.2150, -109.6743,\n",
      "         -104.7673]]), tensor([[-124.7624, -123.2115, -130.5318,  ..., -134.2353, -126.6790,\n",
      "         -125.2399]])), attentions=None, hidden_states=None)\n",
      "====\n",
      "transition_scores:\n",
      "tensor([[-1.2596, -1.5887, -1.9784, -2.1347, -1.2013, -1.2065, -1.4077, -1.2406,\n",
      "         -2.4125, -2.8545, -0.5006, -0.9584, -1.1055, -1.3975, -1.6479, -1.7749,\n",
      "         -0.2831, -0.6819, -0.3247, -0.6229]])\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "draft_outputs = draft_model.generate(draft_inputs.input_ids, max_new_tokens=20, return_dict_in_generate=True, output_scores=True, do_sample=False)\n",
    "target_outputs = target_model.generate(target_inputs.input_ids, max_new_tokens=20, return_dict_in_generate=True, output_scores=True, do_sample=False)\n",
    "print(\"draft_outputs.sequences == target_outputs.sequences:\")\n",
    "print(draft_outputs.sequences == target_outputs.sequences)\n",
    "\n",
    "print(\"draft_outputs.sequences:\")\n",
    "print(draft_tokenizer.batch_decode(draft_outputs.sequences))\n",
    "\n",
    "print(\"target_outputs.sequences:\")\n",
    "print(target_tokenizer.batch_decode(target_outputs.sequences))\n",
    "\n",
    "print(\"draft_outputs:\")\n",
    "print(draft_outputs)\n",
    "print(\"====\")\n",
    "transition_scores = draft_model.compute_transition_scores(\n",
    "    draft_outputs.sequences, draft_outputs.scores, normalize_logits=True\n",
    ")\n",
    "print(\"transition_scores:\")\n",
    "print(transition_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_input_ids.shape: torch.Size([3, 3])\n",
      "additional_input_ids:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3\n",
    "additional_input_ids = torch.arange(K).unsqueeze(0).repeat(3, 1)\n",
    "print(\"additional_input_ids.shape:\", additional_input_ids.shape)\n",
    "print(\"additional_input_ids:\")\n",
    "additional_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,    11,   616,  3290,   318, 13779],\n",
       "        [15496,    11,   616,  3290,   318, 13779],\n",
       "        [15496,    11,   616,  3290,   318, 13779]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tile(draft_inputs.input_ids, (K, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is attention_mask is unnecessary? tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# Verifying: generate(**inputs, ...) == generate(inputs.input_ids, ...)\n",
    "print(\"Is attention_mask is unnecessary?\", torch.all(draft_model.generate(draft_inputs.input_ids, max_new_tokens=20, return_dict_in_generate=True, output_scores=True, do_sample=False).sequences == draft_model.generate(**draft_inputs, max_new_tokens=20, return_dict_in_generate=True, output_scores=True, do_sample=False).sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch import Tensor\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "def print_shapes(obj: Any, level: int = 0):\n",
    "    padding = \"--\" * level\n",
    "    if isinstance(obj, Tensor):\n",
    "        print(f\"{padding}{obj.shape}\")\n",
    "    elif isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            print(f\"{padding}{k}\")\n",
    "            print_shapes(v, level + 1)\n",
    "    elif isinstance(obj, str):\n",
    "        print(f\"{padding}type {type(obj)} is not supported\")\n",
    "    elif isinstance(obj, Iterable):\n",
    "        for x in obj:\n",
    "            print_shapes(x, level + 1)\n",
    "    else:\n",
    "        print(f\"{padding}type {type(obj)} is not supported\")\n",
    "\n",
    "\n",
    "# # Test cases\n",
    "# print_shapes([\"test\", Tensor([1, 2, 3]), {\"key\": Tensor([4, 5, 6])}])\n",
    "# print(\"====\")\n",
    "# print_shapes(\"This is a string\")\n",
    "# print(\"====\")\n",
    "# print_shapes((1, 2, Tensor([7, 8, 9]), {\"nested\": [Tensor([10, 11, 12]), \"string\"]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "assert isinstance(draft_inputs, transformers.tokenization_utils_base.BatchEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences\n",
      "--torch.Size([1, 26])\n",
      "scores\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "print_shapes(draft_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(draft_outputs.scores), len(draft_outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n",
      "tensor(-156.0659)\n",
      "tensor(-112.9710)\n"
     ]
    }
   ],
   "source": [
    "print(draft_outputs.scores[-1].shape)\n",
    "print(draft_outputs.scores[-1].min())\n",
    "print(draft_outputs.scores[-1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft_tokenizer.batch_decode(draft_outputs.sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragged batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = eos_token_id = torch.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see how to create a batch of inputs with different lengths...\n",
      "---\n",
      "Here is how the input_ids look like. It is a tensor of shape (batch_size=1, sequence_length):\n",
      "draft_inputs.input_ids.shape=torch.Size([1, 6])\n",
      "draft_inputs.input_ids=tensor([[15496,    11,   616,  3290,   318, 13779]])\n",
      "---\n",
      "We want to create something similar... First, here is how we can concatenate new tokens to a sequence:\n",
      "seq=tensor([[0., 1., 2.]])\n",
      "seq.shape=torch.Size([1, 3])\n",
      "\n",
      "new_token=tensor([[3.]])\n",
      "new_token.shape=torch.Size([1, 1])\n",
      "\n",
      "dft_seq=tensor([[0., 1., 2., 3.]])\n",
      "dft_seq.shape=torch.Size([1, 4])\n",
      "---\n",
      "=== NESTED TENSORS ===\n",
      "batch=nested_tensor([\n",
      "  tensor([0., 1., 2.]),\n",
      "  tensor([0., 1., 2., 3.])\n",
      "])\n",
      "batch_reversed=nested_tensor([\n",
      "  tensor([0., 1., 2., 3.]),\n",
      "  tensor([0., 1., 2.])\n",
      "])\n",
      "---\n",
      "Unfortunately, converting a nested tensor to a padded tensor is always left-padded:\n",
      "padded_batch=tensor([[0., 1., 2., nan],\n",
      "        [0., 1., 2., 3.]])\n",
      "padded_batch.shape=torch.Size([2, 4])\n",
      "\n",
      "padded_batch_reversed=tensor([[0., 1., 2., 3.],\n",
      "        [0., 1., 2., nan]])\n",
      "padded_batch_reversed.shape=torch.Size([2, 4])\n",
      "---\n",
      "=== LEFT PADDING ===\n",
      "Padding the original sequence to the same length as new_seq:\n",
      "pad_size=1\n",
      "padded_seq=tensor([[nan, 0., 1., 2.]])\n",
      "Creating a batch with padded_seq and new_seq:\n",
      "padded_batch=tensor([[nan, 0., 1., 2.],\n",
      "        [0., 1., 2., 3.]])\n",
      "padded_batch.shape=torch.Size([2, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/b3gdpg7x2yg_rv2dwd3ly30w0000gp/T/ipykernel_7706/2356974417.py:26: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:180.)\n",
      "  batch: Tensor = torch.nested.nested_tensor([seq.squeeze(), dft_seq.squeeze()])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Let's see how to create a batch of inputs with different lengths...\", end=\"\\n---\\n\")\n",
    "print(\"Here is how the input_ids look like. It is a tensor of shape (batch_size=1, sequence_length):\")\n",
    "print(f\"{draft_inputs.input_ids.shape=}\")\n",
    "print(f\"{draft_inputs.input_ids=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"We want to create something similar... First, here is how we can concatenate new tokens to a sequence:\")\n",
    "# Initial sequence and new token\n",
    "seq_len: int = 3\n",
    "seq = torch.arange(seq_len, dtype=torch.float).view(1, -1)  # Shape: (batch_size=1, seq_len=3)\n",
    "new_token = torch.Tensor([3]).view(1, -1)  # Shape: (batch_size=1, seq_len=1)\n",
    "dft_seq = torch.cat((seq, new_token), dim=-1)  # Shape: (batch_size=1, seq_len=4)\n",
    "print(f\"{seq=}\")\n",
    "print(f\"{seq.shape=}\")\n",
    "print()\n",
    "print(f\"{new_token=}\")\n",
    "print(f\"{new_token.shape=}\")\n",
    "print()\n",
    "print(f\"{dft_seq=}\")\n",
    "print(f\"{dft_seq.shape=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"=== NESTED TENSORS ===\")\n",
    "batch: Tensor = torch.nested.nested_tensor([seq.squeeze(), dft_seq.squeeze()])\n",
    "batch_reversed: Tensor = torch.nested.nested_tensor([dft_seq.squeeze(), seq.squeeze()])\n",
    "print(f\"{batch=}\")\n",
    "print(f\"{batch_reversed=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Unfortunately, converting a nested tensor to a padded tensor is always left-padded:\")\n",
    "padded_batch: Tensor = torch.nested.to_padded_tensor(batch, padding=pad_token_id)\n",
    "padded_batch_reversed: Tensor = torch.nested.to_padded_tensor(batch_reversed, padding=pad_token_id)\n",
    "print(f\"{padded_batch=}\")\n",
    "print(f\"{padded_batch.shape=}\")\n",
    "print()\n",
    "print(f\"{padded_batch_reversed=}\")\n",
    "print(f\"{padded_batch_reversed.shape=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"=== LEFT PADDING ===\")\n",
    "print(\"Padding the original sequence to the same length as new_seq:\")\n",
    "pad_size = dft_seq.size(1) - seq.size(1)\n",
    "print(f\"{pad_size=}\")\n",
    "\n",
    "padded_seq = torch.nn.functional.pad(seq, (pad_size, 0), value=pad_token_id)  # Left padding\n",
    "print(f\"{padded_seq=}\")\n",
    "print(\"Creating a batch with padded_seq and new_seq:\")\n",
    "padded_batch = torch.stack([padded_seq.squeeze(), dft_seq.squeeze()], dim=0)\n",
    "print(f\"{padded_batch=}\")\n",
    "print(f\"{padded_batch.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we generate K drafts, the batch size will be K+1:\n",
      "dft_seq=tensor([[10, 11, 12]])\n",
      "dft_seq.shape=torch.Size([1, 3])\n",
      "dft_seq=tensor([[ 0.,  1.,  2., 10., 11., 12.]])\n",
      "---\n",
      "Let's create a batch with the original sequence and K drafts via left padding:\n",
      "padded_batch.shape=torch.Size([4, 6])\n",
      "padded_batch=\n",
      "tensor([[nan, nan, nan, 0., 1., 2.],\n",
      "        [nan, nan, 0., 1., 2., 0.],\n",
      "        [nan, 0., 1., 2., 0., 1.],\n",
      "        [0., 1., 2., 0., 1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# Initial sequence, K drafts, and new sequences\n",
    "print(\"If we generate K drafts, the batch size will be K+1:\")\n",
    "K: int = 3\n",
    "dft_seq: Tensor = torch.arange(10, 10 + K).view(1, -1)  # Shape: (batch_size=1, seq_len=K)\n",
    "print(f\"{dft_seq=}\")\n",
    "print(f\"{dft_seq.shape=}\")\n",
    "dft_seq = torch.cat((seq, dft_seq), dim=-1)  # Shape: (batch_size=1, seq_len=seq_len+K)\n",
    "print(f\"{dft_seq=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Let's create a batch with the original sequence and K drafts via left padding:\")\n",
    "# Initialize a list to hold the padded sequences\n",
    "padded_batch = []\n",
    "\n",
    "# Iteratively pad each sequence\n",
    "for i in range(K + 1):\n",
    "    # Calculate the amount of padding needed\n",
    "    pad_size = dft_seq.size(1) - i - seq.size(1)\n",
    "    \n",
    "    # Create the padded sequence\n",
    "    padded_seq = torch.cat([torch.full((1, pad_size), pad_token_id), seq], dim=-1)\n",
    "    \n",
    "    # Add the draft tokens as needed\n",
    "    if i > 0:\n",
    "        padded_seq = torch.cat([padded_seq, dft_seq[:, :i]], dim=-1)\n",
    "    \n",
    "    # Add the padded sequence to the list\n",
    "    padded_batch.append(padded_seq)\n",
    "\n",
    "# Combine all padded sequences into a batch\n",
    "padded_batch = torch.cat(padded_batch, dim=0)\n",
    "\n",
    "print(f\"{padded_batch.shape=}\")\n",
    "print(\"padded_batch=\")\n",
    "print(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize the above code by pre-allocating the padded_batch:\n",
      "dft_seq=tensor([[ 0.,  1.,  2., 10., 11., 12.]])\n",
      "dft_seq.shape=torch.Size([1, 6])\n",
      "K=3\n",
      "pad_token_id=nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  0.,  1.,  2.],\n",
       "        [nan, nan,  0.,  1.,  2., 10.],\n",
       "        [nan,  0.,  1.,  2., 10., 11.],\n",
       "        [ 0.,  1.,  2., 10., 11., 12.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Optimize the above code by pre-allocating the padded_batch:\")\n",
    "\n",
    "def get_batch(\n",
    "    dft_seq: Tensor,  # A sequence of input_ids of shape (batch_size=1, seq_len + K)\n",
    "    K: int,\n",
    "    pad_token_id,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns a left-padded ragged batch of shape (K + 1, seq_len + K).\n",
    "    \"\"\"\n",
    "    # Infer the length of the original sequence (seq_len)\n",
    "    seq_len = dft_seq.size(1) - K\n",
    "\n",
    "    # Pre-allocate a tensor filled with the pad_token_id for padded_batch\n",
    "    padded_batch = torch.full((K + 1, seq_len + K), pad_token_id)\n",
    "\n",
    "    # Extract the original sequence from dft_seq\n",
    "    seq = dft_seq[:, :seq_len]\n",
    "\n",
    "    # Fill in the appropriate values in the pre-allocated tensor\n",
    "    for i in range(K + 1):\n",
    "        start_idx = K - i\n",
    "        padded_batch[i, start_idx: start_idx + seq_len] = seq\n",
    "        if i > 0:\n",
    "            padded_batch[i, start_idx + seq_len: start_idx + seq_len + i] = dft_seq[:, seq_len:seq_len + i]\n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "print(f\"{dft_seq=}\")\n",
    "print(f\"{dft_seq.shape=}\")\n",
    "print(f\"{K=}\")\n",
    "print(f\"{pad_token_id=}\")\n",
    "padded_batch = get_batch(dft_seq=dft_seq, K=K, pad_token_id=pad_token_id)\n",
    "padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq=tensor([[15496,    11,   616,  3290,   318, 13779]])\n",
      "seq.shape=torch.Size([1, 6])\n",
      "dft_seq=tensor([[15496,    11,   616,  3290,   318, 13779,    13,   314,  1101,   407,\n",
      "          1654,   611,   673,   338,   257, 26188,   393,   407,    13,   314,\n",
      "          1101,   407,  1654,   611,   673,   338]])\n",
      "dft_seq.shape=torch.Size([1, 26])\n",
      "---\n",
      "pad_token_id=50256\n",
      "K=20\n",
      "batch.shape=torch.Size([21, 26])\n",
      "target_tokenizer.batch_decode(batch):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute.',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I',\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not.\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if\",\n",
       " \"<|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she\",\n",
       " \"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq: Tensor = draft_inputs.input_ids\n",
    "dft_seq: Tensor = draft_outputs.sequences\n",
    "print(f\"{seq=}\")\n",
    "print(f\"{seq.shape=}\")\n",
    "print(f\"{dft_seq=}\")\n",
    "print(f\"{dft_seq.shape=}\")\n",
    "\n",
    "print(\"---\")\n",
    "pad_token_id: int = target_tokenizer.pad_token_id or target_tokenizer.eos_token_id\n",
    "print(f\"{pad_token_id=}\")\n",
    "K: int = dft_seq.size(1) - seq.size(1)\n",
    "print(f\"{K=}\")\n",
    "batch: Tensor = get_batch(dft_seq, K=K, pad_token_id=pad_token_id)\n",
    "print(f\"{batch.shape=}\")\n",
    "print(\"target_tokenizer.batch_decode(batch):\")\n",
    "batched_decoded: list[str] = target_tokenizer.batch_decode(batch)\n",
    "batched_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaymody-sps-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
