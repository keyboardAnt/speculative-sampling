{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nadavt/opt/anaconda3/envs/jaymody-sps-env/bin/python\n",
      "Python 3.9.10\n",
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!python --version\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m torch.utils.collect_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate via `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadavt/opt/anaconda3/envs/jaymody-sps-env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50257])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model & Sanity check\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "draft_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "target_model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "draft_inputs = draft_tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "target_inputs = target_tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "# Assert that `draft_tokenizer` and `target_tokenizer` are equivalent\n",
    "assert torch.all(draft_inputs.input_ids == target_inputs.input_ids)\n",
    "assert torch.all(draft_inputs.attention_mask == target_inputs.attention_mask)\n",
    "\n",
    "draft_outputs = draft_model(**draft_inputs, labels=draft_inputs[\"input_ids\"])\n",
    "target_outputs = target_model(**target_inputs, labels=target_inputs[\"input_ids\"])\n",
    "\n",
    "\n",
    "loss = draft_outputs.loss\n",
    "logits = draft_outputs.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "draft_outputs.sequences == target_outputs.sequences:\n",
      "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False]])\n",
      "draft_outputs.sequences:\n",
      "[\"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\"]\n",
      "target_outputs.sequences:\n",
      "[\"Hello, my dog is cute. I'm going to take him to the park today. I'm going to take him to the\"]\n",
      "draft_outputs:\n",
      "GreedySearchDecoderOnlyOutput(sequences=tensor([[15496,    11,   616,  3290,   318, 13779,    13,   314,  1101,   407,\n",
      "          1654,   611,   673,   338,   257, 26188,   393,   407,    13,   314,\n",
      "          1101,   407,  1654,   611,   673,   338]]), scores=(tensor([[-77.4425, -80.4463, -88.0498,  ..., -96.2564, -93.6345, -84.0666]]), tensor([[-142.3193, -142.1797, -144.0388,  ..., -155.6977, -153.8761,\n",
      "         -137.7942]]), tensor([[-175.1097, -174.3555, -180.2891,  ..., -186.5560, -176.5305,\n",
      "         -176.7951]]), tensor([[-149.5665, -149.3980, -153.9603,  ..., -158.9080, -155.3729,\n",
      "         -151.3569]]), tensor([[-146.1682, -147.2101, -157.6489,  ..., -158.1570, -155.2079,\n",
      "         -150.0087]]), tensor([[-139.9639, -140.1997, -149.4295,  ..., -151.1516, -150.9630,\n",
      "         -143.5753]]), tensor([[-111.7725, -111.0601, -118.1015,  ..., -119.4616, -118.6231,\n",
      "         -113.0703]]), tensor([[-144.3919, -142.3804, -151.1539,  ..., -153.2746, -146.0315,\n",
      "         -144.4816]]), tensor([[-141.3286, -140.4888, -146.7292,  ..., -147.8345, -143.9044,\n",
      "         -142.7541]]), tensor([[-117.3899, -116.4772, -121.3260,  ..., -126.3711, -123.0911,\n",
      "         -117.4414]]), tensor([[ -98.5157,  -99.3022, -105.9519,  ..., -112.5625, -109.3629,\n",
      "         -103.0630]]), tensor([[-106.7821, -107.9948, -112.8315,  ..., -116.4186, -113.5325,\n",
      "         -107.8266]]), tensor([[-45.3370, -46.1747, -54.2726,  ..., -61.4791, -59.7055, -50.6116]]), tensor([[-162.0715, -160.2719, -162.9900,  ..., -174.5018, -174.7954,\n",
      "         -156.2698]]), tensor([[-181.4132, -180.2989, -185.5506,  ..., -191.4404, -182.3735,\n",
      "         -182.6374]]), tensor([[-157.1393, -156.6873, -161.9810,  ..., -163.8544, -162.9273,\n",
      "         -158.6823]]), tensor([[-147.5977, -148.1111, -157.4325,  ..., -156.5320, -155.3785,\n",
      "         -150.3270]]), tensor([[-135.2703, -136.3291, -145.7141,  ..., -147.2964, -147.4553,\n",
      "         -139.9984]]), tensor([[-103.0535, -102.4292, -109.0970,  ..., -110.2150, -109.6743,\n",
      "         -104.7673]]), tensor([[-124.7624, -123.2115, -130.5318,  ..., -134.2353, -126.6790,\n",
      "         -125.2399]])), attentions=None, hidden_states=None)\n",
      "====\n",
      "transition_scores:\n",
      "tensor([[-1.2596, -1.5887, -1.9784, -2.1347, -1.2013, -1.2065, -1.4077, -1.2406,\n",
      "         -2.4125, -2.8545, -0.5006, -0.9584, -1.1055, -1.3975, -1.6479, -1.7749,\n",
      "         -0.2831, -0.6819, -0.3247, -0.6229]])\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "draft_outputs = draft_model.generate(draft_inputs.input_ids, max_new_tokens=20, return_dict_in_generate=True, output_scores=True, do_sample=False)\n",
    "target_outputs = target_model.generate(target_inputs.input_ids, max_new_tokens=20, return_dict_in_generate=True, output_scores=True, do_sample=False)\n",
    "print(\"draft_outputs.sequences == target_outputs.sequences:\")\n",
    "print(draft_outputs.sequences == target_outputs.sequences)\n",
    "\n",
    "print(\"draft_outputs.sequences:\")\n",
    "print(draft_tokenizer.batch_decode(draft_outputs.sequences))\n",
    "\n",
    "print(\"target_outputs.sequences:\")\n",
    "print(target_tokenizer.batch_decode(target_outputs.sequences))\n",
    "\n",
    "print(\"draft_outputs:\")\n",
    "print(draft_outputs)\n",
    "print(\"====\")\n",
    "transition_scores = draft_model.compute_transition_scores(\n",
    "    draft_outputs.sequences, draft_outputs.scores, normalize_logits=True\n",
    ")\n",
    "print(\"transition_scores:\")\n",
    "print(transition_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional_input_ids.shape: torch.Size([3, 3])\n",
      "additional_input_ids:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 2],\n",
       "        [0, 1, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K = 3\n",
    "# additional_input_ids = torch.arange(K).unsqueeze(0).repeat(3, 1)\n",
    "# print(\"additional_input_ids.shape:\", additional_input_ids.shape)\n",
    "# print(\"additional_input_ids:\")\n",
    "# additional_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15496,    11,   616,  3290,   318, 13779],\n",
       "        [15496,    11,   616,  3290,   318, 13779],\n",
       "        [15496,    11,   616,  3290,   318, 13779]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tile(draft_inputs.input_ids, (K, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is attention_mask is unnecessary? tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# Verifying: generate(**inputs, ...) == generate(inputs.input_ids, ...)\n",
    "print(\"Is attention_mask is unnecessary?\", torch.all(draft_model.generate(draft_inputs.input_ids, max_new_tokens=20, return_dict_in_generate=True, output_scores=True, do_sample=False).sequences == draft_model.generate(**draft_inputs, max_new_tokens=20, return_dict_in_generate=True, output_scores=True, do_sample=False).sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Any\n",
    "# from torch import Tensor\n",
    "# from collections.abc import Iterable\n",
    "\n",
    "\n",
    "# def print_shapes(obj: Any, level: int = 0):\n",
    "#     padding = \"--\" * level\n",
    "#     if isinstance(obj, Tensor):\n",
    "#         print(f\"{padding}{obj.shape}\")\n",
    "#     elif isinstance(obj, dict):\n",
    "#         for k, v in obj.items():\n",
    "#             print(f\"{padding}{k}\")\n",
    "#             print_shapes(v, level + 1)\n",
    "#     elif isinstance(obj, str):\n",
    "#         print(f\"{padding}type {type(obj)} is not supported\")\n",
    "#     elif isinstance(obj, Iterable):\n",
    "#         for x in obj:\n",
    "#             print_shapes(x, level + 1)\n",
    "#     else:\n",
    "#         print(f\"{padding}type {type(obj)} is not supported\")\n",
    "\n",
    "\n",
    "# # # Test cases\n",
    "# # print_shapes([\"test\", Tensor([1, 2, 3]), {\"key\": Tensor([4, 5, 6])}])\n",
    "# # print(\"====\")\n",
    "# # print_shapes(\"This is a string\")\n",
    "# # print(\"====\")\n",
    "# # print_shapes((1, 2, Tensor([7, 8, 9]), {\"nested\": [Tensor([10, 11, 12]), \"string\"]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "\n",
    "# assert isinstance(draft_inputs, transformers.tokenization_utils_base.BatchEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences\n",
      "--torch.Size([1, 26])\n",
      "scores\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n",
      "----torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "# print_shapes(draft_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(draft_outputs.scores), len(draft_outputs.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50257])\n",
      "tensor(-156.0659)\n",
      "tensor(-112.9710)\n"
     ]
    }
   ],
   "source": [
    "# print(draft_outputs.scores[-1].shape)\n",
    "# print(draft_outputs.scores[-1].min())\n",
    "# print(draft_outputs.scores[-1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draft_tokenizer.batch_decode(draft_outputs.sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ragged batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = eos_token_id = torch.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see how to create a batch of inputs with different lengths...\n",
      "---\n",
      "Here is how the input_ids look like. It is a tensor of shape (batch_size=1, sequence_length):\n",
      "draft_inputs.input_ids.shape=torch.Size([1, 6])\n",
      "draft_inputs.input_ids=tensor([[15496,    11,   616,  3290,   318, 13779]])\n",
      "---\n",
      "We want to create something similar... First, here is how we can concatenate new tokens to a sequence:\n",
      "seq=tensor([[0., 1., 2.]])\n",
      "seq.shape=torch.Size([1, 3])\n",
      "\n",
      "new_token=tensor([[3.]])\n",
      "new_token.shape=torch.Size([1, 1])\n",
      "\n",
      "dft_seq=tensor([[0., 1., 2., 3.]])\n",
      "dft_seq.shape=torch.Size([1, 4])\n",
      "---\n",
      "=== NESTED TENSORS ===\n",
      "batch=nested_tensor([\n",
      "  tensor([0., 1., 2.]),\n",
      "  tensor([0., 1., 2., 3.])\n",
      "])\n",
      "batch_reversed=nested_tensor([\n",
      "  tensor([0., 1., 2., 3.]),\n",
      "  tensor([0., 1., 2.])\n",
      "])\n",
      "---\n",
      "Unfortunately, converting a nested tensor to a padded tensor is always left-padded:\n",
      "padded_batch=tensor([[0., 1., 2., nan],\n",
      "        [0., 1., 2., 3.]])\n",
      "padded_batch.shape=torch.Size([2, 4])\n",
      "\n",
      "padded_batch_reversed=tensor([[0., 1., 2., 3.],\n",
      "        [0., 1., 2., nan]])\n",
      "padded_batch_reversed.shape=torch.Size([2, 4])\n",
      "---\n",
      "=== LEFT PADDING ===\n",
      "Padding the original sequence to the same length as new_seq:\n",
      "pad_size=1\n",
      "padded_seq=tensor([[nan, 0., 1., 2.]])\n",
      "Creating a batch with padded_seq and new_seq:\n",
      "padded_batch=tensor([[nan, 0., 1., 2.],\n",
      "        [0., 1., 2., 3.]])\n",
      "padded_batch.shape=torch.Size([2, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/b3gdpg7x2yg_rv2dwd3ly30w0000gp/T/ipykernel_85363/2356974417.py:26: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:180.)\n",
      "  batch: Tensor = torch.nested.nested_tensor([seq.squeeze(), dft_seq.squeeze()])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Let's see how to create a batch of inputs with different lengths...\", end=\"\\n---\\n\")\n",
    "print(\"Here is how the input_ids look like. It is a tensor of shape (batch_size=1, sequence_length):\")\n",
    "print(f\"{draft_inputs.input_ids.shape=}\")\n",
    "print(f\"{draft_inputs.input_ids=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"We want to create something similar... First, here is how we can concatenate new tokens to a sequence:\")\n",
    "# Initial sequence and new token\n",
    "seq_len: int = 3\n",
    "seq = torch.arange(seq_len, dtype=torch.float).view(1, -1)  # Shape: (batch_size=1, seq_len=3)\n",
    "new_token = torch.Tensor([3]).view(1, -1)  # Shape: (batch_size=1, seq_len=1)\n",
    "dft_seq = torch.cat((seq, new_token), dim=-1)  # Shape: (batch_size=1, seq_len=4)\n",
    "print(f\"{seq=}\")\n",
    "print(f\"{seq.shape=}\")\n",
    "print()\n",
    "print(f\"{new_token=}\")\n",
    "print(f\"{new_token.shape=}\")\n",
    "print()\n",
    "print(f\"{dft_seq=}\")\n",
    "print(f\"{dft_seq.shape=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"=== NESTED TENSORS ===\")\n",
    "batch: Tensor = torch.nested.nested_tensor([seq.squeeze(), dft_seq.squeeze()])\n",
    "batch_reversed: Tensor = torch.nested.nested_tensor([dft_seq.squeeze(), seq.squeeze()])\n",
    "print(f\"{batch=}\")\n",
    "print(f\"{batch_reversed=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Unfortunately, converting a nested tensor to a padded tensor is always left-padded:\")\n",
    "padded_batch: Tensor = torch.nested.to_padded_tensor(batch, padding=pad_token_id)\n",
    "padded_batch_reversed: Tensor = torch.nested.to_padded_tensor(batch_reversed, padding=pad_token_id)\n",
    "print(f\"{padded_batch=}\")\n",
    "print(f\"{padded_batch.shape=}\")\n",
    "print()\n",
    "print(f\"{padded_batch_reversed=}\")\n",
    "print(f\"{padded_batch_reversed.shape=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"=== LEFT PADDING ===\")\n",
    "print(\"Padding the original sequence to the same length as new_seq:\")\n",
    "pad_size = dft_seq.size(1) - seq.size(1)\n",
    "print(f\"{pad_size=}\")\n",
    "\n",
    "padded_seq = torch.nn.functional.pad(seq, (pad_size, 0), value=pad_token_id)  # Left padding\n",
    "print(f\"{padded_seq=}\")\n",
    "print(\"Creating a batch with padded_seq and new_seq:\")\n",
    "padded_batch = torch.stack([padded_seq.squeeze(), dft_seq.squeeze()], dim=0)\n",
    "print(f\"{padded_batch=}\")\n",
    "print(f\"{padded_batch.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we generate K drafts, the batch size will be K+1:\n",
      "dft_seq=tensor([[10, 11, 12]])\n",
      "dft_seq.shape=torch.Size([1, 3])\n",
      "dft_seq=tensor([[ 0.,  1.,  2., 10., 11., 12.]])\n",
      "---\n",
      "Let's create a batch with the original sequence and K drafts via left padding:\n",
      "padded_batch.shape=torch.Size([4, 6])\n",
      "padded_batch=\n",
      "tensor([[nan, nan, nan, 0., 1., 2.],\n",
      "        [nan, nan, 0., 1., 2., 0.],\n",
      "        [nan, 0., 1., 2., 0., 1.],\n",
      "        [0., 1., 2., 0., 1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# Initial sequence, K drafts, and new sequences\n",
    "print(\"If we generate K drafts, the batch size will be K+1:\")\n",
    "K: int = 3\n",
    "dft_seq: Tensor = torch.arange(10, 10 + K).view(1, -1)  # Shape: (batch_size=1, seq_len=K)\n",
    "print(f\"{dft_seq=}\")\n",
    "print(f\"{dft_seq.shape=}\")\n",
    "dft_seq = torch.cat((seq, dft_seq), dim=-1)  # Shape: (batch_size=1, seq_len=seq_len+K)\n",
    "print(f\"{dft_seq=}\")\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Let's create a batch with the original sequence and K drafts via left padding:\")\n",
    "# Initialize a list to hold the padded sequences\n",
    "padded_batch = []\n",
    "\n",
    "# Iteratively pad each sequence\n",
    "for i in range(K + 1):\n",
    "    # Calculate the amount of padding needed\n",
    "    pad_size = dft_seq.size(1) - i - seq.size(1)\n",
    "    \n",
    "    # Create the padded sequence\n",
    "    padded_seq = torch.cat([torch.full((1, pad_size), pad_token_id), seq], dim=-1)\n",
    "    \n",
    "    # Add the draft tokens as needed\n",
    "    if i > 0:\n",
    "        padded_seq = torch.cat([padded_seq, dft_seq[:, :i]], dim=-1)\n",
    "    \n",
    "    # Add the padded sequence to the list\n",
    "    padded_batch.append(padded_seq)\n",
    "\n",
    "# Combine all padded sequences into a batch\n",
    "padded_batch = torch.cat(padded_batch, dim=0)\n",
    "\n",
    "print(f\"{padded_batch.shape=}\")\n",
    "print(\"padded_batch=\")\n",
    "print(padded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize the above code by pre-allocating the padded_batch:\n",
      "dft_seq=tensor([[ 0.,  1.,  2., 10., 11., 12.]])\n",
      "dft_seq.shape=torch.Size([1, 6])\n",
      "K=3\n",
      "pad_token_id=nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan,  0.,  1.,  2.],\n",
       "        [nan, nan,  0.,  1.,  2., 10.],\n",
       "        [nan,  0.,  1.,  2., 10., 11.],\n",
       "        [ 0.,  1.,  2., 10., 11., 12.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Optimize the above code by pre-allocating the padded_batch:\")\n",
    "\n",
    "def get_batch(\n",
    "    dft_seq: Tensor,  # A sequence of input_ids of shape (batch_size=1, seq_len + K)\n",
    "    K: int,\n",
    "    pad_token_id,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Returns a left-padded ragged batch of shape (K + 1, seq_len + K).\n",
    "    \"\"\"\n",
    "    # Infer the length of the original sequence (seq_len)\n",
    "    seq_len = dft_seq.size(1) - K\n",
    "\n",
    "    # Pre-allocate a tensor filled with the pad_token_id for padded_batch\n",
    "    # TODO: Replact `full` with `empty`\n",
    "    padded_batch = torch.full((K + 1, seq_len + K), pad_token_id)\n",
    "\n",
    "    # Extract the original sequence from dft_seq\n",
    "    seq = dft_seq[:, :seq_len]\n",
    "\n",
    "    # Fill in the appropriate values in the pre-allocated tensor\n",
    "    for i in range(K + 1):\n",
    "        start_idx = K - i\n",
    "        padded_batch[i, start_idx: start_idx + seq_len] = seq\n",
    "        if i > 0:\n",
    "            padded_batch[i, start_idx + seq_len: start_idx + seq_len + i] = dft_seq[:, seq_len:seq_len + i]\n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "print(f\"{dft_seq=}\")\n",
    "print(f\"{dft_seq.shape=}\")\n",
    "print(f\"{K=}\")\n",
    "print(f\"{pad_token_id=}\")\n",
    "padded_batch = get_batch(dft_seq=dft_seq, K=K, pad_token_id=pad_token_id)\n",
    "padded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq=tensor([[15496,    11,   616,  3290,   318, 13779]])\n",
      "seq.shape=torch.Size([1, 6])\n",
      "target_tokenizer.batch_decode(seq)=['Hello, my dog is cute']\n",
      "dft_seq=tensor([[15496,    11,   616,  3290,   318, 13779,    13,   314,  1101,   407,\n",
      "          1654,   611,   673,   338,   257, 26188,   393,   407,    13,   314,\n",
      "          1101,   407,  1654,   611,   673,   338]])\n",
      "dft_seq.shape=torch.Size([1, 26])\n",
      "target_tokenizer.batch_decode(dft_seq)=[\"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\"]\n",
      "---\n",
      "pad_token_id=50256\n",
      "K=20\n",
      "batch.shape=torch.Size([21, 26])\n",
      "target_tokenizer.batch_decode(batch):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute.',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I',\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not.\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if\",\n",
       " \"<|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she\",\n",
       " \"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq: Tensor = draft_inputs.input_ids\n",
    "dft_seq: Tensor = draft_outputs.sequences\n",
    "print(f\"{seq=}\")\n",
    "print(f\"{seq.shape=}\")\n",
    "print(f\"{target_tokenizer.batch_decode(seq)=}\")\n",
    "print(f\"{dft_seq=}\")\n",
    "print(f\"{dft_seq.shape=}\")\n",
    "print(f\"{target_tokenizer.batch_decode(dft_seq)=}\")\n",
    "\n",
    "print(\"---\")\n",
    "pad_token_id: int = target_tokenizer.pad_token_id or target_tokenizer.eos_token_id\n",
    "print(f\"{pad_token_id=}\")\n",
    "K: int = dft_seq.size(1) - seq.size(1)\n",
    "print(f\"{K=}\")\n",
    "batch: Tensor = get_batch(dft_seq, K=K, pad_token_id=pad_token_id)\n",
    "print(f\"{batch.shape=}\")\n",
    "print(\"target_tokenizer.batch_decode(batch):\")\n",
    "batched_decoded: list[str] = target_tokenizer.batch_decode(batch)\n",
    "batched_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_batch.shape=torch.Size([21, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute and',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. He',\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if you\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a dog\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or a\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not,\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. She\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if\",\n",
       " \"<|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she\",\n",
       " \"<|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\",\n",
       " \"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's a\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if we can generate one additional token for each sequence in the batch\n",
    "new_batch: Tensor = target_model.generate(batch, max_new_tokens=1, do_sample=False)\n",
    "print(f\"{new_batch.shape=}\")\n",
    "assert new_batch.size(0) == batch.size(0), \"The batch size should not change\"\n",
    "assert new_batch.size(1) == batch.size(1) + 1, \"The sequence length should increase by 1\"\n",
    "target_tokenizer.batch_decode(new_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute and',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. He',\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if you\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a dog\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or a\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not,\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. She\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if\",\n",
       " \"<|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she\",\n",
       " \"<|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\",\n",
       " \"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's a\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tokenizer.batch_decode(target_model.generate(batch, max_new_tokens=1, return_dict_in_generate=True, output_scores=True, do_sample=False).sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt='Hello, my dog is cute'\n",
      "padded_prompt='<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute'\n",
      "=== GENERATE ONE TOKEN ===\n",
      "generated='Hello, my dog is cute.'\n",
      "generated_from_padded='<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute and'\n",
      "=== REMOVE LEFT PADDING ===\n",
      "generated_from_padded_and_remove_padding_post_generation='Hello, my dog is cute and'\n",
      "In general, padding affects the generation results.\n",
      "In this case, the generations are the same:  False\n",
      "generated:\n",
      "Hello, my dog is cute.\n",
      "generated_from_padded_and_remove_padding_post_generation:\n",
      "Hello, my dog is cute and\n"
     ]
    }
   ],
   "source": [
    "def generate_one_token_with_target_model(prompt: str) -> str:\n",
    "    return target_tokenizer.batch_decode(target_model.generate(target_tokenizer.encode(prompt, return_tensors=\"pt\"), max_new_tokens=1, do_sample=False))\n",
    "\n",
    "prompt: str = \"Hello, my dog is cute\"\n",
    "padded_prompt: str = \"<|endoftext|>\" * 20 + prompt\n",
    "print(f\"{prompt=}\")\n",
    "print(f\"{padded_prompt=}\")\n",
    "\n",
    "print(\"=== GENERATE ONE TOKEN ===\")\n",
    "generated: str = generate_one_token_with_target_model(prompt)[0]\n",
    "generated_from_padded: str = generate_one_token_with_target_model(padded_prompt)[0]\n",
    "print(f\"{generated=}\")\n",
    "print(f\"{generated_from_padded=}\")\n",
    "\n",
    "\n",
    "def remove_left_padding(string: str, pad_token: str) -> str:\n",
    "    while string.startswith(pad_token):\n",
    "        string = string[len(pad_token):]\n",
    "    return string\n",
    "\n",
    "print(\"=== REMOVE LEFT PADDING ===\")\n",
    "generated_from_padded_and_remove_padding_post_generation: str = remove_left_padding(generated_from_padded, pad_token=\"<|endoftext|>\")\n",
    "print(f\"{generated_from_padded_and_remove_padding_post_generation=}\")\n",
    "\n",
    "print(\"In general, padding affects the generation results.\")\n",
    "is_same: bool = generated == generated_from_padded_and_remove_padding_post_generation\n",
    "print(\"In this case, the generations are the same: \", is_same)\n",
    "if not is_same:\n",
    "    print(\"generated:\")\n",
    "    print(generated)\n",
    "    print(\"generated_from_padded_and_remove_padding_post_generation:\")\n",
    "    print(generated_from_padded_and_remove_padding_post_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8 µs ± 7.06 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.empty((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.59 µs ± 20.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.full((10, 10), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_attention_mask(n_rows: int, n_cols: int) -> Tensor:\n",
    "    # Create a column vector [0, 1, 2, ..., rows-1]\n",
    "    i = torch.arange(n_rows).view(-1, 1)\n",
    "    # Create a row vector [0, 1, 2, ..., cols-1]\n",
    "    j = torch.arange(n_cols)\n",
    "    return (i + j > n_rows - 2).int()\n",
    "\n",
    "get_attention_mask(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the attention mask makes the padding invisible to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt:\n",
      "shape:  torch.Size([1, 6])\n",
      "prompt='Hello, my dog is cute'\n",
      "\n",
      "padded_prompt:\n",
      "padded_prompt='<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute'\n",
      "shape:  torch.Size([1, 26])\n",
      "\n",
      "N=20\n"
     ]
    }
   ],
   "source": [
    "prompt_encoded: Tensor = target_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "padded_prompt_encoded: Tensor = target_tokenizer.encode(padded_prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(\"prompt:\")\n",
    "print(\"shape: \", prompt_encoded.shape)\n",
    "print(f\"{prompt=}\")\n",
    "print()\n",
    "print(\"padded_prompt:\")\n",
    "print(f\"{padded_prompt=}\")\n",
    "print(\"shape: \", padded_prompt_encoded.shape)\n",
    "print()\n",
    "N: int = padded_prompt_encoded.size(1) - prompt_encoded.size(1)\n",
    "print(f\"{N=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1]])\n",
      "attention_mask.count_nonzero()=tensor(6)\n"
     ]
    }
   ],
   "source": [
    "attention_mask: Tensor = torch.zeros_like(padded_prompt_encoded)\n",
    "attention_mask[:, -prompt_encoded.size(1):] = 1\n",
    "print(f\"{attention_mask=}\")\n",
    "print(f\"{attention_mask.count_nonzero()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_attention_mask(n_rows=batch.size(0), n_cols=batch.size(1)).shape=torch.Size([21, 26])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{get_attention_mask(n_rows=batch.size(0), n_cols=batch.size(1)).shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How to `forward` efficiently? (i.e., so that the model returns only the logits)\n",
    "\n",
    "A (see below): Set `use_cache=False` and `return_dict=False`. By default, `output_hidden_states=False` and `output_attentions=False`.\n",
    "\n",
    "```python\n",
    "outputs = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    use_cache=False,\n",
    "    return_dict=False\n",
    ")\n",
    "```\n",
    "Then, `len(outputs) == 1` and `outputs[0].shape == (batch_size, sequence_length, vocab_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded_prompt_encoded.shape=torch.Size([1, 26])\n",
      "type(outputs)=<class 'tuple'>\n",
      "len(outputs)=1\n",
      "type(outputs[0])=<class 'torch.Tensor'>\n",
      "type(outputs[-1])=<class 'torch.Tensor'>\n",
      "outputs[0].shape=torch.Size([1, 26, 50257])\n"
     ]
    }
   ],
   "source": [
    "# To test the output of the target model:\n",
    "print(f\"{padded_prompt_encoded.shape=}\")\n",
    "outputs = target_model(\n",
    "    input_ids=padded_prompt_encoded,\n",
    "    attention_mask=attention_mask,\n",
    "    use_cache=False,\n",
    "    # output_hidden_states=False,\n",
    "    # output_attentions=False,\n",
    "    return_dict=False,\n",
    ")\n",
    "print(f\"{type(outputs)=}\")\n",
    "print(f\"{len(outputs)=}\")\n",
    "print(f\"{type(outputs[0])=}\")\n",
    "print(f\"{type(outputs[-1])=}\")\n",
    "print(f\"{outputs[0].shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How to `generate` so that the model returns only the new tokens?\n",
    "\n",
    "A: \n",
    "```python\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=num_of_new_tokens,\n",
    "    do_sample=False,\n",
    "    use_cache=False,\n",
    "    return_dict=False,\n",
    ")\n",
    "```\n",
    "* If `input_ids` is a single sequence (rather than a batch of size > 1) of shape `(batch_size=1, sequence_length)`, then `len(outputs) == 1` and `outputs[0].shape == (sequence_length + num_of_new_tokens,)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The outputs below is a `Tensor` rather than a `tuple`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded_prompt_encoded.shape=torch.Size([1, 26])\n",
      "type(outputs)=<class 'torch.Tensor'>\n",
      "len(outputs)=1\n",
      "type(outputs[0])=<class 'torch.Tensor'>\n",
      "type(outputs[-1])=<class 'torch.Tensor'>\n",
      "outputs[0].shape=torch.Size([27])\n",
      "target_tokenizer.batch_decode(outputs[0])=['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', 'Hello', ',', ' my', ' dog', ' is', ' cute', '.']\n"
     ]
    }
   ],
   "source": [
    "# `padded_prompt_encoded` is a single sequence (namely, batch of size 1)\n",
    "print(f\"{padded_prompt_encoded.shape=}\")\n",
    "outputs = target_model.generate(\n",
    "    input_ids=padded_prompt_encoded,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=1,\n",
    "    do_sample=False,\n",
    "    use_cache=False,\n",
    "    return_dict=False,\n",
    ")\n",
    "print(f\"{type(outputs)=}\")\n",
    "print(f\"{len(outputs)=}\")\n",
    "print(f\"{type(outputs[0])=}\")\n",
    "print(f\"{type(outputs[-1])=}\")\n",
    "print(f\"{outputs[0].shape=}\")\n",
    "print(f\"{target_tokenizer.batch_decode(outputs[0])=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Otherwise (`input_ids` is a batch of size > 1), `len(outputs) == batch_size` and `outputs[i].shape == (sequence_length + num_of_new_tokens,)` for each `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_more_than_one.shape=torch.Size([3, 26])\n",
      "type(outputs)=<class 'torch.Tensor'>\n",
      "len(outputs)=3\n",
      "type(outputs[0])=<class 'torch.Tensor'>\n",
      "type(outputs[-1])=<class 'torch.Tensor'>\n",
      "outputs[0].shape=torch.Size([27])\n",
      "target_tokenizer.batch_decode(outputs[0])=['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', 'Hello', ',', ' my', ' dog', ' is', ' cute', ' and']\n",
      "outputs=tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         15496,    11,   616,  3290,   318, 13779,   290],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         15496,    11,   616,  3290,   318, 13779,   290],\n",
      "        [50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         15496,    11,   616,  3290,   318, 13779,   290]])\n"
     ]
    }
   ],
   "source": [
    "# `batch_size_more_than_one` is a batch of size 3\n",
    "batch_size_more_than_one: Tensor = torch.repeat_interleave(padded_prompt_encoded, repeats=3, dim=0)\n",
    "print(f\"{batch_size_more_than_one.shape=}\")\n",
    "outputs = target_model.generate(\n",
    "    input_ids=batch_size_more_than_one,\n",
    "    attention_mask=torch.ones_like(batch_size_more_than_one),\n",
    "    max_new_tokens=1,\n",
    "    do_sample=False,\n",
    "    use_cache=False,\n",
    "    return_dict=False,\n",
    ")\n",
    "print(f\"{type(outputs)=}\")\n",
    "print(f\"{len(outputs)=}\")\n",
    "print(f\"{type(outputs[0])=}\")\n",
    "print(f\"{type(outputs[-1])=}\")\n",
    "print(f\"{outputs[0].shape=}\")\n",
    "print(f\"{target_tokenizer.batch_decode(outputs[0])=}\")\n",
    "print(f\"{outputs=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the attention mask with `generate` and a batch of size > 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape=torch.Size([21, 26])\n",
      "target_tokenizer.batch_decode(batch):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute.',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I',\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not.\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if\",\n",
       " \"<|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she\",\n",
       " \"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{batch.shape=}\")\n",
    "print(\"target_tokenizer.batch_decode(batch):\")\n",
    "target_tokenizer.batch_decode(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.shape=torch.Size([21, 26])\n",
      "target_tokenizer.batch_decode(batch):\n",
      "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute', '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute.', '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I', \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not.\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm\", \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not\", \"<|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure\", \"<|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if\", \"<|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she\", \"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\"]\n",
      "attention_mask.shape=torch.Size([21, 26])\n",
      "attention_mask=tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]], dtype=torch.int32)\n",
      "type(outputs)=<class 'torch.Tensor'>\n",
      "outputs.shape=torch.Size([21, 27])\n",
      "We know the target model needs to generate:\n",
      "\"\"\"\n",
      "Hello, my dog is cute. I'm going to take him to the park today. I'm going to take him to the\n",
      "\"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute.',\n",
       " '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I',\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm going\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if you\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's cute\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a dog\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or a\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not,\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure\",\n",
       " \"<|endoftext|><|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if\",\n",
       " \"<|endoftext|><|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she\",\n",
       " \"<|endoftext|>Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's\",\n",
       " \"Hello, my dog is cute. I'm not sure if she's a puppy or not. I'm not sure if she's a\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{batch.shape=}\")\n",
    "print(\"target_tokenizer.batch_decode(batch):\")\n",
    "print(target_tokenizer.batch_decode(batch))\n",
    "attention_mask = get_attention_mask(n_rows=batch.size(0), n_cols=batch.size(1))\n",
    "print(f\"{attention_mask.shape=}\")\n",
    "print(f\"{attention_mask=}\")\n",
    "outputs = target_model.generate(\n",
    "    input_ids=batch,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=1,\n",
    "    do_sample=False,\n",
    "    use_cache=False,\n",
    "    return_dict=False\n",
    ")\n",
    "print(f\"{type(outputs)=}\")\n",
    "print(f\"{outputs.shape=}\")\n",
    "print(\"We know the target model needs to generate:\")\n",
    "print('\"\"\"')\n",
    "print(\"Hello, my dog is cute. I'm going to take him to the park today. I'm going to take him to the\")\n",
    "print('\"\"\"')\n",
    "target_tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaymody-sps-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
